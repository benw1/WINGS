#!/usr/bin/env python
"""
Contains the Job class definition

Please note that this module is private. The Job class is
available in the main ``wpipe`` namespace - use that instead.
"""
from .constants import LOGPRINT_TIMESTAMP
from .core import sys, logging, datetime, pd, si
from .core import make_yield_session_if_not_cached, make_query_rtn_upd
from .core import initialize_args, wpipe_to_sqlintf_connection, in_session
from .core import as_int, split_path
from .core import PARSER
from .proxies import ChildrenProxy
from .OptOwner import OptOwner

__all__ = ['Job', 'JOBINITSTATE', 'JOBSUBMSTATE', 'JOBCOMPSTATE', 'JOBEXPISTATE']

JOBINITSTATE = "Initialized"
JOBSUBMSTATE = "Submitted"
JOBCOMPSTATE = "Completed"
JOBEXPISTATE = "Expired"

KEYID_ATTR = 'job_id'
UNIQ_ATTRS = ['task_id', 'config_id', 'firing_event_id', 'attempt']
CLASS_LOW = split_path(__file__)[1].lower()


def _in_session(**local_kw):
    return in_session('_%s' % CLASS_LOW, **local_kw)


_check_in_cache = make_yield_session_if_not_cached(KEYID_ATTR, UNIQ_ATTRS, CLASS_LOW)

_query_return_and_update_cached_row = make_query_rtn_upd(CLASS_LOW, KEYID_ATTR, UNIQ_ATTRS)


class Job(OptOwner):
    """
        Represents a submitted job of a WINGS pipeline.

        Call signatures::

            Job(event, configuration=event.config,
                task=configuration.dummy_task, node=None,
                attempt=1, options={})
            Job(keyid=PARSER.job_id)
            Job(_job)

        When __new__ is called, it queries the database for an existing
        row in the `jobs` table via `sqlintf` using the given signature.
        If the row exists, it retrieves its corresponding `sqlintf.Job`
        object, otherwise it creates a new row via a new `sqlintf.Job`
        instance. This `sqlintf.Job` object is then wrapped under the
        hidden attribute `Job._job` in the new instance of this `Job`
        class generated by __new__.

        All jobs are uniquely identified by their task, their configuration,
        their firing event and their attempt number, but alternatively, the
        constructor can take as sole argument either:
         - the primary key id of the corresponding `jobs` table row
         - the `sqlintf.Job` object interfacing that table row
        It can also be called without argument if the python script importing
        wpipe was ran with the command-line argument --job/-j referring to a
        valid job primary key id, in which case the consequently constructed
        job will correspond to that job.

        After the instantiation of __new__ is completed, if a dictionary of
        options was given to the constructor, the __init__ method constructs
        a set of Option objects owned by the job.

        Parameters
        ----------
        event : Event object
            Firing Event owning this job.
        configuration : Configuration object
            Configuration owning this job - defaults to event.config.
        task : Task object
            Task owning this job - defaults to config.dummy_task.
        node : Node object
            Node owning this job - defaults to None.
        attempt : int
            Job attempt number - defaults to 1.
        options : dict
            Dictionary of options to associate to the job.
        keyid : int
            Primary key id of the table row.
        _job : sqlintf.Job object exposing SQL interface
            Corresponding sqlintf object interfacing the table row.

        Attributes
        ----------
        parents : tuple(Task, Configuration, Node, Event)
            Points to a tuple of attributes self.task, self.config, self.node
            and self.firing_event.
        attempt : int
            Job attempt number.
        state : string
            State of this job.
        job_id : int
            Primary key id of the table row.
        timestamp : datetime.datetime object
            Timestamp of last access to table row.
        starttime : datetime.datetime object
            Timestamp of job starting time.
        endtime : datetime.datetime object
            Timestamp of job ending time.
        is_active : boolean
            True if job is active, False if not.
        has_completed : boolean
            True if job has ended, False if not.
        has_expired : boolean
            True if job has expired, False if not.
        task_changed : boolean
            True if task was modified since job started, False if not.
        task_id : int
            Primary key id of the table row of parent task.
        task : Task object
            Task object corresponding to parent task.
        has_a_node : boolean
            True if Job object is associated to a Node object, False if not.
        node_id : int
            Primary key id of the table row of parent node.
        node : Node object
            Node object corresponding to parent node.
        config_id : int
            Primary key id of the table row of parent configuration.
        config : Configuration object
            Configuration object corresponding to parent configuration.
        pipeline_id : int
            Primary key id of the table row of parent pipeline.
        pipeline : Pipeline object
            Pipeline object corresponding to parent pipeline.
        target_id : int
            Primary key id of the table row of parent target.
        target : Target object
            Target object corresponding to parent target.
        firing_event_id : int
            Primary key id of the table row of parent event.
        firing_event : Event object
            Event object corresponding to parent event.
        child_events : core.ChildrenProxy object
            List of Event objects owned by the job.
        optowner_id : int
            Points to attribute job_id.
        options : core.DictLikeChildrenProxy object
            Dictionary of Option objects owned by the job.

        Notes
        -----
        Job objects are at the heart of Wpipe functionalities, as they handle
        the pipeline jobbing. A Job is constructed with a combination of
        parent Wpipe objects among 4 of them:
         - a Task object is always required, representing the task that the
           job is running,
         - a Node object that represents the node on which the job is running,
         - in most cases, an Event object for the firing event that started
           the job run,
         - and lastly, a Configuration object representing a target and its
           configuration which the job runs with.
        Alternatively, especially when writing tasks for the pipeline, the Job
        object running that task can be accessed from within by simply using
        the Job constructor without arguments.

        Every Pipeline objects are constructed with a default dummy job meant
        to start the pipeline. This particular job constitutes the only case
        in which a Job object comes with only a Task + Node parent combination
        as it is not launched by a particular firing event and is not meant to
        work on any target configuration (generally, these must be initialized
        by the very first task to be called). All other Job objects that are
        not such dummy job always require to construct a firing Event object,
        and in most cases, a target Configuration object.

        One can give each of these 4 objects in any order as arguments of the
        Job class constructor, or use the Job-generating object method of one
        of the parent object, providing the 3 other objects as arguments:

        >>> new_job = wp.Job(my_task, my_node, my_event, my_config)
        or
        >>> new_job = my_task.job(my_node, my_event, my_config)
        or
        >>> new_job = my_node.job(my_task, my_event, my_config)
        or
        >>> new_job = my_event.fired_job(my_task, my_node, my_config)
        or
        >>> new_job = my_config.job(my_task, my_node, my_event)

        Once the Job object is constructed, 2 methods are important for the
        pipeline run: logprint and child_event
         - Job.logprint allows the logging of a job in logfiles named after
           that job, its task and its firing event,
         - Job.child_event is the Event-generating object method of Job, which
           handles the starting of new jobs from an existing job,
    """
    __cache__ = pd.DataFrame(columns=[KEYID_ATTR]+UNIQ_ATTRS+[CLASS_LOW])

    @classmethod
    def _check_in_cache(cls, kind, loc):
        return _check_in_cache(cls, kind, loc)

    @classmethod
    def _sqlintf_instance_argument(cls):
        if hasattr(cls, '_%s' % CLASS_LOW):
            for _session in cls._check_in_cache(kind='keyid',
                                                loc=getattr(cls, '_%s' % CLASS_LOW).get_id()):
                pass

    def __new__(cls, *args, **kwargs):
        if hasattr(cls, '_inst'):
            old_cls_inst = cls._inst
            delattr(cls, '_inst')
        else:
            old_cls_inst = None
        cls._to_cache = {}
        # checking if given argument is sqlintf object or existing id
        cls._job = args[0] if len(args) else as_int(PARSER.parse_known_args()[0].job_id)
        if not isinstance(cls._job, si.Job):
            keyid = kwargs.get('id', cls._job)
            if isinstance(keyid, int):
                for session in cls._check_in_cache(kind='keyid', loc=keyid):
                    cls._job = session.query(si.Job).filter_by(id=keyid).one()
            else:
                # gathering construction arguments
                wpargs, args, kwargs = initialize_args(args, kwargs, nargs=1)
                event = kwargs.get('event', wpargs.get('Event', None))
                config = kwargs.get('config',
                                    wpargs.get('Configuration',
                                               event.config if event is not None else
                                               None))
                task = kwargs.get('task', wpargs.get('Task',
                                                     config.dummy_task if config is not None else
                                                     None))
                node = kwargs.get('node',
                                  wpargs.get('Node', None))
                attempt = kwargs.get('attempt', 1 if args[0] is None else args[0])
                # querying the database for existing row or create
                for session in cls._check_in_cache(kind='args', loc=(task.task_id,
                                                                     None if config is None else config.config_id,
                                                                     None if event is None else event.event_id,
                                                                     attempt)):
                    for retry in session.retrying_nested():
                        with retry:
                            this_nested = retry.retry_state.begin_nested()
                            cls._job = this_nested.session.query(si.Job).with_for_update(). \
                                filter_by(task_id=task.task_id)
                            if config is not None:
                                cls._job = cls._job. \
                                    filter_by(config_id=config.config_id)
                            if event is not None:
                                cls._job = cls._job. \
                                    filter_by(firing_event_id=event.event_id)
                            cls._job = cls._job. \
                                filter_by(attempt=attempt).one_or_none()
                            if cls._job is None:
                                cls._job = si.Job(attempt=attempt,
                                                  state=JOBINITSTATE)
                                task._task.jobs.append(cls._job)
                                if config is not None:
                                    config._configuration.jobs.append(cls._job)
                                if event is not None:
                                    event._event.fired_jobs.append(cls._job)
                                if node is not None:
                                    node._node.jobs.append(cls._job)
                                this_nested.commit()
                            else:
                                this_nested.rollback()
                            retry.retry_state.commit()
        else:
            cls._sqlintf_instance_argument()
        # verifying if instance already exists and return
        wpipe_to_sqlintf_connection(cls, 'Job')
        # add instance to cache dataframe
        if cls._to_cache:
            cls._to_cache[CLASS_LOW] = cls._inst
            cls.__cache__.loc[len(cls.__cache__)] = cls._to_cache
        new_cls_inst = cls._inst
        delattr(cls, '_inst')
        if old_cls_inst is not None:
            cls._inst = old_cls_inst
        return new_cls_inst
    
    @classmethod
    def _return_cached_instances(cls):
        return [getattr(obj, '_%s' % CLASS_LOW) for obj in cls.__cache__[CLASS_LOW]]

    @_in_session()
    def __init__(self, *args, **kwargs):
        if not hasattr(self, '_child_events_proxy'):
            self._child_events_proxy = ChildrenProxy(self._job, 'child_events', 'Event')
        if not hasattr(self, '_log_dp'):
            if not self.config_is_none:
                logpath = self.target.datapath + '/log_' + self.config.name
                logowner = self.config
            else:
                logpath = self.pipeline.pipe_root
                logowner = self.pipeline
            logfile = self.task.name + '_j' + str(self.job_id) + '.log'
            self._log_dp = logowner.dataproduct(filename=logfile, relativepath=logpath, group='log')
        if not hasattr(self, '_optowner'):
            self._optowner = self._job
        super(Job, self).__init__(kwargs.get('options', {}))

    @_in_session()
    def __repr__(self):
        cls = self.__class__.__name__
        description = ', '.join([(f"{prop}={getattr(self, prop)}") for prop in [KEYID_ATTR]+UNIQ_ATTRS])
        return f'{cls}({description})'

    @classmethod
    def select(cls, *args, **kwargs):
        """
        Returns a list of Job objects fulfilling the kwargs filter.

        Parameters
        ----------
        kwargs
            Refer to :class:`sqlintf.Job` for parameters.

        Returns
        -------
        out : list of Job object
            list of objects fulfilling the kwargs filter.
        """
        for session in si.begin_session():
            with session as session:
                cls._temp = session.query(si.Job).filter_by(**kwargs)
                for arg in args:
                    cls._temp = cls._temp.filter(arg)
                return list(map(cls, cls._temp.all()))

    @property
    def parents(self):
        """
        tuple(:obj:`Task`, :obj:`Configuration`, :obj:`Node`, :obj:`Event`):
        Points to a tuple of attributes self.task, self.config, self.node and
        self.firing_event.
        """
        return self.task, self.config, self.node, self.firing_event

    @property
    @_in_session()
    def attempt(self):
        """
        int: Job attempt number.
        """
        return self._job.attempt

    @property
    @_in_session()
    def state(self):
        """
        str: State of this job.
        """
        self._session.refresh(self._job)
        return self._job.state

    @state.setter
    @_in_session()
    def state(self, state):
        self._job.state = state
        self.update_timestamp()
        # self._job.timestamp = datetime.datetime.utcnow()
        # self._session.commit()

    @property
    @_in_session()
    def job_id(self):
        """
        int: Primary key id of the table row.
        """
        return self._job.id

    @property
    @_in_session()
    def starttime(self):
        """
        :obj:`datetime.datetime`: Timestamp of job starting time.
        """
        self._session.refresh(self._job)
        return self._job.starttime

    @property
    @_in_session()
    def endtime(self):
        """
        :obj:`datetime.datetime`: Timestamp of job ending time.
        """
        self._session.refresh(self._job)
        return self._job.endtime

    @property
    def not_submitted(self):
        """
        bool: True if job is in an init state, False if not.
        """
        return self.state == JOBINITSTATE

    @property
    def is_active(self):
        """
        bool: True if job is active, False if not.
        """
        return self.state == JOBSUBMSTATE

    @property
    def has_completed(self):
        """
        bool: True if job has ended, False if not.
        """
        return self.state == JOBCOMPSTATE

    @property
    def has_expired(self):
        """
        bool: True if job has expired, False if not.
        """
        return self.state == JOBEXPISTATE

    @property
    def task_changed(self):
        """
        bool: True if task was modified since job started, False if not.
        """
        if self.starttime is None:
            return True
        else:
            return self.task.last_modification_timestamp > self.starttime

    @property
    @_in_session()
    def task_id(self):
        """
        int: Primary key id of the table row of parent task.
        """
        return self._job.task_id

    @property
    @_in_session()
    def task(self):
        """
        :obj:`Task`: Task object corresponding to parent task.
        """
        if hasattr(self._job.task, '_wpipe_object'):
            return self._job.task._wpipe_object
        else:
            from .Task import Task
            return Task(self._job.task)

    @property
    @_in_session()
    def node_id(self):
        """
        int: Primary key id of the table row of parent node.
        """
        self._session.refresh(self._job)
        return self._job.node_id

    @property
    @_in_session()
    def has_a_node(self):
        """
        bool: True if Job object is associated to a Node object, False if not.
        """
        self._session.refresh(self._job)
        return self._job.node is not None

    @property
    @_in_session()
    def node(self):
        """
        :obj:`Node`: Node object corresponding to parent node.
        """
        if self.has_a_node:
            if hasattr(self._job.node, '_wpipe_object'):
                return self._job.node._wpipe_object
            else:
                from .Node import Node
                return Node(self._job.node)

    @node.setter
    @_in_session()
    def node(self, node):
        if self.has_a_node:
            raise AttributeError("can't set attribute")
        else:
            self._session.add(node._node)
            node._node.jobs.append(self._job)
            self._session.commit()

    @property
    @_in_session()
    def config_is_none(self):
        return self._job.config is None

    @property
    @_in_session()
    def config_id(self):
        """
        int: Primary key id of the table row of parent configuration.
        """
        return self._job.config_id

    @property
    @_in_session()
    def config(self):
        """
        :obj:`Configuration`: Configuration object corresponding to parent
        configuration.
        """
        if not self.config_is_none:
            if hasattr(self._job.config, '_wpipe_object'):
                return self._job.config._wpipe_object
            else:
                from .Configuration import Configuration
                return Configuration(self._job.config)

    @property
    def pipeline_id(self):
        """
        int: Primary key id of the table row of parent pipeline.
        """
        return self.task.pipeline_id

    @property
    def pipeline(self):
        """
        :obj:`Pipeline`: Pipeline object corresponding to parent pipeline.
        """
        return self.task.pipeline

    @property
    def target_id(self):
        """
        int: Primary key id of the table row of parent target.
        """
        return self.config.target_id

    @property
    def target(self):
        """
        :obj:`Target`: Target object corresponding to parent target.
        """
        return self.config.target

    @property
    @_in_session()
    def firing_event_id(self):
        """
        int: Primary key id of the table row of parent event.
        """
        self._session.refresh(self._job)
        return self._job.firing_event_id

    @property
    @_in_session()
    def firing_event(self):
        """
        :obj:`Event`: Event object corresponding to parent event.
        """
        if self._job.firing_event is not None:
            if hasattr(self._job.firing_event, '_wpipe_object'):
                return self._job.firing_event._wpipe_object
            else:
                from .Event import Event
                return Event(self._job.firing_event)

    @property
    def parent_job_id(self):
        """
        int: Primary key id of the table row of parent job.
        """
        return self.firing_event.parent_job_id

    @property
    def parent_job(self):
        """
        :obj:`Job`: Job object corresponding to parent job.
        """
        return self.firing_event.parent_job

    @property
    def child_events(self):
        """
        :obj:`core.ChildrenProxy`: List of Event objects owned by the job.
        """
        return self._child_events_proxy

    def child_event(self, *args, **kwargs):
        """
        Returns an event owned by the job.

        Parameters
        ----------
        kwargs
            Refer to :class:`Event` for parameters.

        Returns
        -------
        event : :obj:`Event`
            Event corresponding to given kwargs.
        """
        from .Event import Event
        return Event(self, *args, **kwargs)

    def logprint(self, log_text=None):
        """
        Log given text in a log dataproduct.

        Parameters
        ----------
        log_text : str
            Text to write.
        """
        if log_text is not None:
            with self._log_dp.open("a") as log:
                if LOGPRINT_TIMESTAMP:
                    log.write(f"{datetime.datetime.utcnow().isoformat()}: ")
                log.write(f"{log_text}\n")
        return self._log_dp

    @_in_session()
    def _starting_todo(self, logprint=True):
        if not self.has_a_node:
            from . import DefaultNode
            self.node = DefaultNode
        if logprint:
            logprint = self.logprint()
            sys.stdout = sys.stderr = logprint.open("a")
            logging.basicConfig(filename=logprint.path, format="%(asctime)s %(levelname)s %(name)s %(message)s")
        self.state = JOBSUBMSTATE
        self._job.starttime = datetime.datetime.utcnow()
        self.update_timestamp()
        # self._job.timestamp = datetime.datetime.utcnow()
        # self._session.commit()

    @_in_session()
    def _ending_todo(self):
        if hasattr(sys, "last_value"):
            self.state = repr(sys.last_value)
        else:
            self.state = JOBCOMPSTATE
            self._job.endtime = datetime.datetime.utcnow()
        self.update_timestamp()
        # self._job.timestamp = datetime.datetime.utcnow()
        # self._session.commit()

    @_in_session()
    def expire(self):
        self.state = JOBEXPISTATE
        self._job.endtime = datetime.datetime.utcnow()
        self.update_timestamp()
        # self._job.timestamp = datetime.datetime.utcnow()
        # self._session.commit()

    def reset(self):
        """
        Reset job.
        """
        self._job.starttime = None
        self._job.endtime = None
        self._log_dp.remove_data()
        self.child_events.delete()
        self.state = JOBINITSTATE

    def delete(self):
        """
        Delete corresponding row from the database.
        """
        self._log_dp.delete()
        self.child_events.delete()
        super(Job, self).delete()
        self.__class__.__cache__ = self.__cache__[self.__cache__[CLASS_LOW] != self]
